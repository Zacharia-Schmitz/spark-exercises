{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 101\n",
    "\n",
    "In this lesson we will cover the basics of working with spark dataframes, and\n",
    "show how spark dataframes are different from the pandas dataframes we have\n",
    "been working with.\n",
    "\n",
    "While spark dataframes might superficially look like pandas dataframes, and\n",
    "even share some of the same methods and syntax, it is important to keep in\n",
    "mind they are 2 seperate types of objects, and, while spark and pandas code\n",
    "might look superficially similar, it tends to be semantically very different.\n",
    "\n",
    "We'll begin by creating the spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new session:\n",
    "session = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://madeleinesmbp2.localdomain:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10777d8d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataframes\n",
    "\n",
    "Spark can convert any pandas dataframe into a spark dataframe with a simple\n",
    "method call. For this lesson, we will use this functionality to demonstrate\n",
    "the differences between spark and pandas dataframes and explore how to work\n",
    "with spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n group\n",
       "0  0     c\n",
       "1  1     c\n",
       "2  2     b\n",
       "3  3     a\n",
       "4  4     c"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1349)\n",
    "\n",
    "pandas_dataframe = pd.DataFrame(\n",
    "    dict(n=np.arange(20), group=np.random.choice(list('abc'), 20))\n",
    ")\n",
    "pandas_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start with a simple pandas dataset, and now we will convert it to a\n",
    "spark dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[n: bigint, group: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, while we do see the column names, we don't see the data in the\n",
    "dataframe like we would with a pandas dataframe. This is because spark is\n",
    "*lazy*, in that it won't show us values until it has to. For the purposes of\n",
    "looking at the first few rows of our data, we can use the `.show` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  n|group|\n",
      "+---+-----+\n",
      "|  0|    c|\n",
      "|  1|    c|\n",
      "|  2|    b|\n",
      "|  3|    a|\n",
      "|  4|    c|\n",
      "+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.head()\n",
    "# In the process of constructing actions here,\n",
    "# note that the final .show() is going to be the thing that will \n",
    "# execute and show the action of what we are doing\n",
    "# secondarily, note that everything is going to display in text-based\n",
    "# ascii-like tabular data formatting\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like pandas dataframes, spark dataframes have a .describe method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----+\n",
      "|summary|                n|group|\n",
      "+-------+-----------------+-----+\n",
      "|  count|               20|   20|\n",
      "|   mean|              9.5| null|\n",
      "| stddev|5.916079783099616| null|\n",
      "|    min|                0|    a|\n",
      "|    max|               19|    c|\n",
      "+-------+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which, also like pandas, returns another dataframe. However, since this is a\n",
    "spark dataframe, we have to explicitly show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  n|group|\n",
      "+---+-----+\n",
      "|  0|    c|\n",
      "|  1|    c|\n",
      "|  2|    b|\n",
      "+---+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default spark will show the first 20 rows, but we can specify how many we\n",
    "want by passing a number to `.show`.\n",
    "\n",
    "Let's use some different data so that we have a more robust dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data\n",
    "mpg = data('mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>model</th>\n",
       "      <th>displ</th>\n",
       "      <th>year</th>\n",
       "      <th>cyl</th>\n",
       "      <th>trans</th>\n",
       "      <th>drv</th>\n",
       "      <th>cty</th>\n",
       "      <th>hwy</th>\n",
       "      <th>fl</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>4</td>\n",
       "      <td>auto(l5)</td>\n",
       "      <td>f</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>4</td>\n",
       "      <td>manual(m5)</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>manual(m6)</td>\n",
       "      <td>f</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>auto(av)</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>6</td>\n",
       "      <td>auto(l5)</td>\n",
       "      <td>f</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  manufacturer model  displ  year  cyl       trans drv  cty  hwy fl    class\n",
       "1         audi    a4    1.8  1999    4    auto(l5)   f   18   29  p  compact\n",
       "2         audi    a4    1.8  1999    4  manual(m5)   f   21   29  p  compact\n",
       "3         audi    a4    2.0  2008    4  manual(m6)   f   20   31  p  compact\n",
       "4         audi    a4    2.0  2008    4    auto(av)   f   21   30  p  compact\n",
       "5         audi    a4    2.8  1999    6    auto(l5)   f   16   26  p  compact"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = session.createDataFrame(mpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another difference from pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+---+\n",
      "|             model|displ|cyl|\n",
      "+------------------+-----+---+\n",
      "|                a4|  1.8|  4|\n",
      "|                a4|  1.8|  4|\n",
      "|                a4|  2.0|  4|\n",
      "|                a4|  2.0|  4|\n",
      "|                a4|  2.8|  6|\n",
      "|                a4|  2.8|  6|\n",
      "|                a4|  3.1|  6|\n",
      "|        a4 quattro|  1.8|  4|\n",
      "|        a4 quattro|  1.8|  4|\n",
      "|        a4 quattro|  2.0|  4|\n",
      "|        a4 quattro|  2.0|  4|\n",
      "|        a4 quattro|  2.8|  6|\n",
      "|        a4 quattro|  2.8|  6|\n",
      "|        a4 quattro|  3.1|  6|\n",
      "|        a4 quattro|  3.1|  6|\n",
      "|        a6 quattro|  2.8|  6|\n",
      "|        a6 quattro|  3.1|  6|\n",
      "|        a6 quattro|  4.2|  8|\n",
      "|c1500 suburban 2wd|  5.3|  8|\n",
      "|c1500 suburban 2wd|  5.3|  8|\n",
      "+------------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# looking at a subset of columns:\n",
    "# pandas: df[[''model', 'displ', 'cyl']]\n",
    "mpg.select(mpg.model, mpg.displ, mpg.cyl).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this expression would produce a Series of values from a pandas\n",
    "dataframe, for a spark dataframe this produces a Column object, which is an\n",
    "object that represents a vertical slice of a dataframe, but does not contain\n",
    "the data itself.\n",
    "\n",
    "One way to use our column objects is to use them in combination with the\n",
    "`.select` method. `.select` is very powerful, and lets us specify what data we\n",
    "want to see in the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|displ|  class|\n",
      "+-----+-------+\n",
      "|  1.8|compact|\n",
      "|  1.8|compact|\n",
      "|  2.0|compact|\n",
      "+-----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg['displ'], mpg['class']).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+\n",
      "|displ|cyl|  class|\n",
      "+-----+---+-------+\n",
      "|  1.8|  4|compact|\n",
      "|  1.8|  4|compact|\n",
      "|  2.0|  4|compact|\n",
      "|  2.0|  4|compact|\n",
      "|  2.8|  6|compact|\n",
      "+-----+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.displ, mpg['cyl'], 'class').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, notice that we don't see any data, instead we see the new dataframe\n",
    "that is produced. To see the actual data, we'll again need to use `.show`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .select will give us the set of columns we want to see\n",
    "# we can reference the columns with dot notation just like pandas Series\n",
    "# and if we want to see the results of our action, we can chain a .show()\n",
    "# at the end of the command\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our column objects support a numer of operations, including the arithmetic\n",
    "operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(cty + 1)'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the city mileage and add 1:\n",
    "# a single additive transformation of the column of data\n",
    "# does not require a shuffle, just wants to add 1 to everything\n",
    "mpg.cty + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get back a column that represents the values from the original `hwy`\n",
    "column with 1 added to them. To actually see this data, we'd need to select it\n",
    "and show the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|cty|(cty + 1)|\n",
      "+---+---------+\n",
      "| 18|       19|\n",
      "| 21|       22|\n",
      "| 20|       21|\n",
      "| 21|       22|\n",
      "| 16|       17|\n",
      "+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select('cty', mpg.cty + 1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a column object, we can use the `.alias` method to rename it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "|cty|city_plus_one|\n",
      "+---+-------------+\n",
      "| 18|           19|\n",
      "| 21|           22|\n",
      "| 20|           21|\n",
      "| 21|           22|\n",
      "| 16|           17|\n",
      "+---+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select('cty', (mpg.cty + 1).alias('city_plus_one')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also store column objects in variables and reference them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# col1: the hwy column from the mpg dataframe, \n",
    "# aliased as highway_mileage\n",
    "# col2: the calculation of the highway column divided by two, \n",
    "# aliased as highway_mileage_halved\n",
    "col1 = mpg.hwy.alias('highway_mileage')\n",
    "col2 = (mpg.hwy / 2).alias('highway_mileage_halved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Column<'hwy AS highway_mileage'>,\n",
       " Column<'(hwy / 2) AS highway_mileage_halved'>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col1, col2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|highway_mileage|highway_mileage_halved|\n",
      "+---------------+----------------------+\n",
      "|             29|                  14.5|\n",
      "|             29|                  14.5|\n",
      "|             31|                  15.5|\n",
      "|             30|                  15.0|\n",
      "|             26|                  13.0|\n",
      "+---------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(col1, col2).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways to create columns\n",
    "\n",
    "In addition to the syntax we've seen above, we can create columns with the\n",
    "`col` and `expr` functions from `pyspark.sql.functions` module.\n",
    "\n",
    "### `col`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpg.select(mpg.hwy).show(5)\n",
    "hwy_col = F.col('hwy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|hwy|\n",
      "+---+\n",
      "| 29|\n",
      "| 29|\n",
      "| 31|\n",
      "| 30|\n",
      "| 26|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(hwy_col).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can mix and match the syntax we use, and the column object produced by the\n",
    "`col` function is the same as the the previous column object we saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `hwy` cannot be resolved. Did you mean one of the following? [`n`, `group`].;\n'Project ['hwy]\n+- LogicalRDD [n#0L, group#1], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mselect(hwy_col)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/miniconda3/envs/homebase/lib/python3.11/site-packages/pyspark/sql/dataframe.py:3036\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2992\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2993\u001b[0m \n\u001b[1;32m   2994\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3034\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3036\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jcols(\u001b[38;5;241m*\u001b[39mcols))\n\u001b[1;32m   3037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/miniconda3/envs/homebase/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/homebase/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `hwy` cannot be resolved. Did you mean one of the following? [`n`, `group`].;\n'Project ['hwy]\n+- LogicalRDD [n#0L, group#1], false\n"
     ]
    }
   ],
   "source": [
    "df.select(hwy_col).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mpg,\n",
    "# select the column hwy, alias it\n",
    "# select the column cty, alias it,\n",
    "# select the column avg_column, alias it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_col = (2 / (\n",
    "    1 / F.col('hwy').alias('highway_mileage')\n",
    ") + (\n",
    "    1 / F.col('cty').alias('city_mileage')\n",
    ")).alias('avg_mileage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|      avg_mileage|\n",
      "+-----------------+\n",
      "|58.05555555555556|\n",
      "|58.04761904761905|\n",
      "|            62.05|\n",
      "|60.04761904761905|\n",
      "|          52.0625|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(avg_col).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a variable named `avg_column` that represents the average of the\n",
    "highway and city mileage of each vehicle. This variable is created by using\n",
    "the `col` function to produce pyspark Column objects and using the arithmetic\n",
    "operators to combine them.\n",
    "\n",
    "Next we select the original highway and city mileage columns, in addition to\n",
    "our new average mileage column. We demonstrate the `col` function to select\n",
    "the `hwy` column and refer to the city mileage column with the `df.cty`\n",
    "syntax we saw previously. We also give all of our columns more readable\n",
    "aliases before showing the resulting dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `expr`\n",
    "\n",
    "The `expr` function is more powerful than `col`. It does everything `col` does\n",
    "and more. `expr` returns the same type of column object, but allows us to\n",
    "express manipulations to the column within the string that defines the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "hwy_plus = F.expr('hwy + 1 AS highway_plus_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|highway_plus_one|\n",
      "+----------------+\n",
      "|              30|\n",
      "|              30|\n",
      "|              32|\n",
      "|              31|\n",
      "|              27|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(hwy_plus).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------+\n",
      "|hwy|(hwy + 1)|highway|highay_plus|\n",
      "+---+---------+-------+-----------+\n",
      "| 29|       30|     29|         30|\n",
      "| 29|       30|     29|         30|\n",
      "| 31|       32|     31|         32|\n",
      "| 30|       31|     30|         31|\n",
      "| 26|       27|     26|         27|\n",
      "+---+---------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    # the same as `col`\n",
    "    F.expr('hwy'),\n",
    "    # an arithmetic expression\n",
    "    F.expr('hwy + 1'),\n",
    "    # using an alias\n",
    "    F.expr('hwy AS highway'),\n",
    "    # a combination of the above\n",
    "    F.expr('hwy + 1 AS highay_plus')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all the columns created below are identical, and which syntax to use\n",
    "is merely a style choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+-------+-----------+\n",
      "|hwy|(hwy + 1)|(hwy + 1)|highway|higway_plus|\n",
      "+---+---------+---------+-------+-----------+\n",
      "| 29|       30|       30|     29|         30|\n",
      "| 29|       30|       30|     29|         30|\n",
      "| 31|       32|       32|     31|         32|\n",
      "| 30|       31|       31|     30|         31|\n",
      "| 26|       27|       27|     26|         27|\n",
      "+---+---------+---------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    # the same as `col`\n",
    "    mpg['hwy'],\n",
    "    # an arithmetic expression\n",
    "    F.col('hwy') + 1,\n",
    "    F.expr('hwy + 1'),\n",
    "    # using an alias\n",
    "    mpg['hwy'].alias('highway'),\n",
    "    # a combination of the above\n",
    "    F.expr('hwy + 1 AS higway_plus')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "As we've seen through the column definitions, spark is very flexible and\n",
    "allows us many different ways to express ourselves. Another way that is fairly\n",
    "different than what we've seen above is through **spark SQL**, which lets us\n",
    "write SQL queries against our spark dataframes.\n",
    "\n",
    "In order to start using spark SQL, we'll first \"register\" the table with\n",
    "spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.createOrReplaceTempView('mpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a sql query against the `mpg` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|cty|\n",
      "+---+\n",
      "| 18|\n",
      "| 21|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql('''\n",
    "SELECT cty FROM mpg LIMIT 2''').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the resulting value is another dataframe. As we know, in order to\n",
    "view the values in a dataframe, we need to use `.show`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|arithmetic_avg|\n",
      "+--------------+\n",
      "|          23.5|\n",
      "|          25.0|\n",
      "|          25.5|\n",
      "|          25.5|\n",
      "|          21.0|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql('''\n",
    "SELECT (hwy + cty) / 2 AS arithmetic_avg FROM mpg''').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that all of the methods for creating / manipulating\n",
    "dataframes outlined above are the same in terms of performance as well. All of\n",
    "the resulting dataframes get turned into the same spark code that gets\n",
    "executed on the JVM, so it really is just a style choice as to which to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Casting\n",
    "\n",
    "We can view the types of the column in our dataframe in one of two ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('manufacturer', 'string'),\n",
       " ('model', 'string'),\n",
       " ('displ', 'double'),\n",
       " ('year', 'bigint'),\n",
       " ('cyl', 'bigint'),\n",
       " ('trans', 'string'),\n",
       " ('drv', 'string'),\n",
       " ('cty', 'bigint'),\n",
       " ('hwy', 'bigint'),\n",
       " ('fl', 'string'),\n",
       " ('class', 'string')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- displ: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- cyl: long (nullable = true)\n",
      " |-- trans: string (nullable = true)\n",
      " |-- drv: string (nullable = true)\n",
      " |-- cty: long (nullable = true)\n",
      " |-- hwy: long (nullable = true)\n",
      " |-- fl: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .printSchema() is similar to a .info() call\n",
    "mpg.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both provide the same information.\n",
    "\n",
    "To convert from one type to another, we can use the `.cast` method on a\n",
    "column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|hwy|\n",
      "+---+\n",
      "| 29|\n",
      "| 29|\n",
      "| 31|\n",
      "| 30|\n",
      "+---+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas: .astype(int)\n",
    "mpg.select(mpg.hwy.cast('string')).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if a value is not able to be converted, it will be replaced with\n",
    "null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|manufacturer|\n",
      "+------------+\n",
      "|        null|\n",
      "|        null|\n",
      "|        null|\n",
      "|        null|\n",
      "|        null|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.manufacturer.cast('int')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Built-in Functions\n",
    "\n",
    "We've used the `col` and `expr` functions, but there are many other functions\n",
    "within the `pyspark.sql.functions` module, all of which operate on pyspark\n",
    "dataframe columns. Here we'll demonstrate several:\n",
    "\n",
    "- `concat`: to concatenate strings\n",
    "- `sum`: to sum a group\n",
    "- `avg`: to take the average of a group\n",
    "- `min`: to find the minimum\n",
    "- `max`: to find the maximum\n",
    "\n",
    "**_Note that importing the `sum` function directly will override the built-in\n",
    "`sum` function._** This means you will get an error if you try to sum a list\n",
    "of numbers, because `sum` will refernce the pyspark `sum` function, which\n",
    "works with pyspark dataframe columns, while the built-in `sum` function works\n",
    "with lists of numbers. The same holds true for the built in `min` and `max`\n",
    "functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can chain an alias on a concat function call\n",
    "# we need to cast cylinders with a lit() function call to cast it as a string literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-----+\n",
      "|   highway_miles|manufacturer|model|\n",
      "+----------------+------------+-----+\n",
      "|29 miles per gal|        audi|   a4|\n",
      "|29 miles per gal|        audi|   a4|\n",
      "|31 miles per gal|        audi|   a4|\n",
      "|30 miles per gal|        audi|   a4|\n",
      "|26 miles per gal|        audi|   a4|\n",
      "+----------------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: The pyspark avg and mean functions are aliases of eachother\n",
    "mpg.select(\n",
    "    F.concat(\n",
    "        (mpg.hwy).cast('string').alias('do_you_see_this'), \n",
    "        F.lit(' miles per gal')).alias('highway_miles'),\n",
    "    F.col('manufacturer'),\n",
    "    F.expr('model')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Parses the expression string into the column that it represents\n",
       "\n",
       ".. versionadded:: 1.5.0\n",
       "\n",
       ".. versionchanged:: 3.4.0\n",
       "    Supports Spark Connect.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "str : str\n",
       "    expression defined in string.\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`~pyspark.sql.Column`\n",
       "    column representing the expression.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> df = spark.createDataFrame([[\"Alice\"], [\"Bob\"]], [\"name\"])\n",
       ">>> df.select(\"name\", expr(\"length(name)\")).show()\n",
       "+-----+------------+\n",
       "| name|length(name)|\n",
       "+-----+------------+\n",
       "|Alice|           5|\n",
       "|  Bob|           3|\n",
       "+-----+------------+\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/homebase/lib/python3.11/site-packages/pyspark/sql/functions.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F.expr?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!tip \"`pyspark` imports\"\n",
    "    In this lesson we will explicitly import any functions from `pyspark.sql.functions` that we use, but it very common to see something like:\n",
    "    \n",
    "    ```\n",
    "    from pyspark.sql.functions import *\n",
    "    ```\n",
    "    \n",
    "    which will import *all* of the functions from the `pyspark.sql.functions` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use a string literal as part of our select, we'll need to use the\n",
    "`lit` function, otherwise spark will try to resolve our string as a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select the concatenation of the number of cylinders (the value from\n",
    "the `cyl` column) and the string literal \" cylinders\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More `pyspark` Functions for String Manipulation\n",
    "\n",
    "Let's take a look at a couple more functions for string manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import regexp_extract, regexp_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to demonstrate these functions we'll create a dataframe with some text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|address                                      |\n",
      "+---------------------------------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf = session.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"address\": [\n",
    "                \"600 Navarro St ste 600, San Antonio, TX 78205\",\n",
    "                \"3130 Broadway St, San Antonio, TX 78209\",\n",
    "                \"303 Pearl Pkwy, San Antonio, TX 78215\",\n",
    "                \"1255 SW Loop 410, San Antonio, TX 78227\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "textdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `regexp_extract` function lets us specify a regular expression with at least one capture group, and create a new column based on the contents of a capture group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------+------------------+\n",
      "|address                                      |street_no|street            |\n",
      "+---------------------------------------------+---------+------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|600      |Navarro St ste 600|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |3130     |Broadway St       |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |303      |Pearl Pkwy        |\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |1255     |SW Loop 410       |\n",
      "+---------------------------------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in my select call:\n",
    "# address, the column as it stands\n",
    "# regexp_extract, acting as a re.extract, aliasing that with a .alias method \n",
    "textdf.select(\n",
    "    \"address\",\n",
    "    F.regexp_extract('address', r'^(\\d+)', 1).alias(\"street_no\"),\n",
    "    F.regexp_extract('address', r'^\\d+\\s([\\w\\s]+)', 1).alias(\"street\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the first argument to `regexp_extract` is the name of the string column to extract from, the second argument is the regular expression itself, and the last argument specifies which capture group we want to use. If, for example, our regular expression had 2 capture groups in it and we wanted the contents of the 2nd group, we would specify a 2 here.\n",
    "\n",
    "In addition to `regexp_extract`, `regexp_replace` lets us make substitutions based on a regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------------------+\n",
      "|address                                      |city_state_zip       |\n",
      "+---------------------------------------------+---------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|San Antonio, TX 78205|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |San Antonio, TX 78209|\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |San Antonio, TX 78215|\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |San Antonio, TX 78227|\n",
      "+---------------------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.select(\n",
    "    \"address\",\n",
    "    # regexp_replace('subject', 'pattern', 'replacement)\n",
    "    F.regexp_replace('address', r'^.*?,\\s', '').alias(\"city_state_zip\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example above, we obtain just the city, state, and zip code of the address by replacing everything up to the first comma with an empty string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.filter` and `.where`\n",
    "\n",
    "Spark provides two dataframe methods, `.filter` and `.where`, which both allow\n",
    "us to select a subset of the rows of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|     model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|        a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|        a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|        a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|        a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|a4 quattro|  1.8|1999|  4|manual(m5)|  4| 18| 26|  p|compact|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas:\n",
    "# mpg[mpg.cyl == 4]\n",
    "mpg.filter(mpg.cyl == 4).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|     model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|        a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|        a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|        a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|        a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|a4 quattro|  1.8|1999|  4|manual(m5)|  4| 18| 26|  p|compact|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.filter(mpg.cyl == 4).where(F.col('class') == 'compact').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When and Otherwise\n",
    "\n",
    "Similar to an `IF` in Excel, `CASE...WHEN` in SQL, or `np.where` in python,\n",
    "spark provides a `when` function.\n",
    "\n",
    "The `when` function lets us specify a condition, and a value to produce if\n",
    "that condition is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/25 11:01:00 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----------------+------------------+-----------------+-----------------+----------+---+------------------+-----------------+----+-------+\n",
      "|summary|manufacturer|            model|             displ|             year|              cyl|     trans|drv|               cty|              hwy|  fl|  class|\n",
      "+-------+------------+-----------------+------------------+-----------------+-----------------+----------+---+------------------+-----------------+----+-------+\n",
      "|  count|         234|              234|               234|              234|              234|       234|234|               234|              234| 234|    234|\n",
      "|   mean|        null|             null| 3.471794871794871|           2003.5|5.888888888888889|      null|4.0|16.858974358974358|23.44017094017094|null|   null|\n",
      "| stddev|        null|             null|1.2919590310839348|4.509646313320439|1.611534484684289|      null|0.0| 4.255945678889394|5.954643441166446|null|   null|\n",
      "|    min|        audi|      4runner 4wd|               1.6|             1999|                4|  auto(av)|  4|                 9|               12|   c|2seater|\n",
      "|    max|  volkswagen|toyota tacoma 4wd|               7.0|             2008|                8|manual(m6)|  r|                35|               44|   r|    suv|\n",
      "+-------+------------+-----------------+------------------+-----------------+-----------------+----------+---+------------------+-----------------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|good_witch_or_bad_witch|\n",
      "+-----------------------+\n",
      "|           good_mileage|\n",
      "|           good_mileage|\n",
      "|           good_mileage|\n",
      "|           good_mileage|\n",
      "|            bad_mileage|\n",
      "+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    F.when(mpg.hwy > 27, 'good_mileage'\n",
    "          ).otherwise(\n",
    "        'bad_mileage').alias(\n",
    "        'good_witch_or_bad_witch'\n",
    "    )).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|hwy|good_witch_bad_witch|\n",
      "+---+--------------------+\n",
      "| 29|        good_mileage|\n",
      "| 29|        good_mileage|\n",
      "| 31|        good_mileage|\n",
      "| 30|        good_mileage|\n",
      "| 26|         bad_mileage|\n",
      "| 26|         bad_mileage|\n",
      "| 27|         bad_mileage|\n",
      "| 26|         bad_mileage|\n",
      "| 25|         bad_mileage|\n",
      "| 28|        good_mileage|\n",
      "| 27|         bad_mileage|\n",
      "| 25|         bad_mileage|\n",
      "| 25|         bad_mileage|\n",
      "| 25|         bad_mileage|\n",
      "| 25|         bad_mileage|\n",
      "| 24|         bad_mileage|\n",
      "| 25|         bad_mileage|\n",
      "| 23|         bad_mileage|\n",
      "| 20|         bad_mileage|\n",
      "| 15|         bad_mileage|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql('''\n",
    "SELECT hwy,\n",
    "CASE WHEN \n",
    "hwy > 27 THEN 'good_mileage'\n",
    "ELSE 'bad_mileage'\n",
    "END good_witch_bad_witch\n",
    "FROM mpg\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that if the condition we specified is false, `null` will be\n",
    "produced. Instead of null, we can specify a value to use if our condition is\n",
    "false with the `.otherwise` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify multiple conditions, we can chain `.when` calls. The first\n",
    "condition that is met will be the value that is used, and if none of the\n",
    "conditions are met the value specified in the `.otherwise` will be used (or\n",
    "`null` if you don't provide a `.otherwise`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|good_witch_or_bad_witch|\n",
      "+-----------------------+\n",
      "|           good_mileage|\n",
      "|           good_mileage|\n",
      "|           good_mileage|\n",
      "|           good_mileage|\n",
      "|             ok_mileage|\n",
      "|             ok_mileage|\n",
      "|             ok_mileage|\n",
      "|             ok_mileage|\n",
      "|             ok_mileage|\n",
      "|           good_mileage|\n",
      "|             ok_mileage|\n",
      "|             ok_mileage|\n",
      "|             ok_mileage|\n",
      "|             ok_mileage|\n",
      "|             ok_mileage|\n",
      "|             ok_mileage|\n",
      "|             ok_mileage|\n",
      "|             ok_mileage|\n",
      "|             ok_mileage|\n",
      "|            bad_mileage|\n",
      "+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    F.when(mpg.hwy > 27, 'good_mileage'\n",
    "          ).when(\n",
    "    mpg.hwy > 18, 'ok_mileage')\n",
    "    .otherwise(\n",
    "        'bad_mileage').alias(\n",
    "        'good_witch_or_bad_witch'\n",
    "    )).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here that a car with a `displ` of 1.8 matches both conditions we\n",
    "specified, but `small` is produced because it is associated with the first\n",
    "matching condition. For any value between 2 and 3, `medium` will be produced,\n",
    "and anything larger than 3 will produce `large`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting and Ordering\n",
    "\n",
    "Spark lets us sort the rows in our dataframe by one or multiple columns with\n",
    "two methods: `.sort`, and `.orderBy`. `.sort` and `.orderBy` are aliases of\n",
    "each other and do the exact same thing. Like other methods we've seen, `.sort`\n",
    "takes in a Column object or a string that is the name of a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|manufacturer|       model|displ|year|cyl|     trans|drv|cty|hwy| fl|     class|\n",
      "+------------+------------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|     hyundai|     tiburon|  2.0|1999|  4|  auto(l4)|  f| 19| 26|  r|subcompact|\n",
      "|      toyota|     corolla|  1.8|1999|  4|  auto(l3)|  f| 24| 30|  r|   compact|\n",
      "|       honda|       civic|  1.6|1999|  4|manual(m5)|  f| 28| 33|  r|subcompact|\n",
      "|      toyota|     corolla|  1.8|1999|  4|  auto(l4)|  f| 24| 33|  r|   compact|\n",
      "|      subaru|forester awd|  2.5|1999|  4|manual(m5)|  4| 18| 25|  r|       suv|\n",
      "|      toyota|     corolla|  1.8|1999|  4|manual(m5)|  f| 26| 35|  r|   compact|\n",
      "|       honda|       civic|  1.6|1999|  4|  auto(l4)|  f| 24| 32|  r|subcompact|\n",
      "|      toyota|     corolla|  1.8|2008|  4|manual(m5)|  f| 28| 37|  r|   compact|\n",
      "|   chevrolet|      malibu|  2.4|1999|  4|  auto(l4)|  f| 19| 27|  r|   midsize|\n",
      "|      toyota|     corolla|  1.8|2008|  4|  auto(l4)|  f| 26| 35|  r|   compact|\n",
      "+------------+------------+-----+----+---+----------+---+---+---+---+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note that this time when specifying ascending or descending,\n",
    "# we pass it directly on the column rather than as a kwarg\n",
    "#  additionally, we are just passing an mpg.sort here, not an mpg.select\n",
    "mpg.sort(mpg.cyl.asc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, values are sorted in ascending order. To sort in descending order,\n",
    "we can use the `.desc` method on any Column object, or the `desc` function\n",
    "from `pyspark.sql.functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import asc, desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify sorting by multiple columns, we provide each column as a separate\n",
    "argument to `.sort`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|manufacturer|     model|displ|year|cyl|     trans|drv|cty|hwy| fl|     class|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|  volkswagen|     jetta|  1.9|1999|  4|manual(m5)|  f| 33| 44|  d|   compact|\n",
      "|  volkswagen|new beetle|  1.9|1999|  4|manual(m5)|  f| 35| 44|  d|subcompact|\n",
      "|  volkswagen|new beetle|  1.9|1999|  4|  auto(l4)|  f| 29| 41|  d|subcompact|\n",
      "|      toyota|   corolla|  1.8|2008|  4|manual(m5)|  f| 28| 37|  r|   compact|\n",
      "|       honda|     civic|  1.8|2008|  4|  auto(l5)|  f| 25| 36|  r|subcompact|\n",
      "|       honda|     civic|  1.8|2008|  4|  auto(l5)|  f| 24| 36|  c|subcompact|\n",
      "|      toyota|   corolla|  1.8|1999|  4|manual(m5)|  f| 26| 35|  r|   compact|\n",
      "|      toyota|   corolla|  1.8|2008|  4|  auto(l4)|  f| 26| 35|  r|   compact|\n",
      "|       honda|     civic|  1.8|2008|  4|manual(m5)|  f| 26| 34|  r|subcompact|\n",
      "|      toyota|   corolla|  1.8|1999|  4|  auto(l4)|  f| 24| 33|  r|   compact|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.sort(mpg.cyl.asc()).sort(mpg.hwy.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will first reverse alphabetically by number of cylinders from lowest to highest, then by the vehicle's highway\n",
    "mileage, from greatest to smallest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Aggregating\n",
    "\n",
    "To aggregate our data by group, we can use the `.groupBy` method. Like with\n",
    "`.select`, we can pass either Column objects or strings that are column names\n",
    "to `.groupBy`. All of the expressions below are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x16dfe8250>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.groupBy(mpg.cyl)\n",
    "mpg.groupBy(F.col('cyl'))\n",
    "mpg.groupBy('cyl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is grouped, we need to specify an aggregation. We can use one of\n",
    "the aggregate functions we imported earlier, alond with a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|cyl|avg_hwy_mpg|\n",
      "+---+-----------+\n",
      "|  6|      22.82|\n",
      "|  8|      17.63|\n",
      "|  4|       28.8|\n",
      "|  5|      28.75|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.groupby('cyl').agg(F.round((F.avg(mpg.hwy)),2).alias('avg_hwy_mpg')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To group by multiple columns, pass each of the columns a a separate argument\n",
    "to `.groupBy` (Note that this is different from pandas, where we would need to\n",
    "pass a list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------------------+\n",
      "|cyl|   trans|          avg(hwy)|\n",
      "+---+--------+------------------+\n",
      "|  6|auto(s6)|              26.0|\n",
      "|  4|auto(s6)|             28.25|\n",
      "|  8|auto(l4)|16.733333333333334|\n",
      "|  4|auto(l5)|              31.0|\n",
      "|  4|auto(av)|              30.5|\n",
      "+---+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# two level grop-by is passed as two positional arguments\n",
    "# inside of the groupBy call\n",
    "mpg.groupBy('cyl', 'trans').agg(F.avg(mpg.hwy)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to `.groupBy`, we can use `.rollup`, which will do the same\n",
    "aggregations, but will also include the overall total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+\n",
      "| cyl|     trans|          avg(hwy)|\n",
      "+----+----------+------------------+\n",
      "|   6|  auto(l5)|           21.4375|\n",
      "|   6|  auto(av)|              26.0|\n",
      "|   8|      null| 17.62857142857143|\n",
      "|   6|manual(m6)|              22.6|\n",
      "|   4|  auto(av)|              30.5|\n",
      "|   6|      null| 22.82278481012658|\n",
      "|   4|manual(m6)|29.571428571428573|\n",
      "|   8|  auto(l4)|16.733333333333334|\n",
      "|null|      null| 23.44017094017094|\n",
      "|   6|  auto(s6)|              26.0|\n",
      "|   4|      null| 28.80246913580247|\n",
      "|   4|manual(m5)|29.272727272727273|\n",
      "|   4|  auto(s6)|             28.25|\n",
      "|   6|manual(m5)|22.666666666666668|\n",
      "|   8|  auto(s6)|              20.4|\n",
      "|   4|  auto(l5)|              31.0|\n",
      "|   8|manual(m6)|              20.0|\n",
      "|   6|  auto(l4)|22.689655172413794|\n",
      "|   4|  auto(l4)|            27.625|\n",
      "|   4|  auto(l3)|              27.0|\n",
      "+----+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.rollup('cyl', 'trans').agg(F.avg(mpg.hwy)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the null value in `cyl` indicates the total count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the example above, the null row represents the overall average highway\n",
    "mileage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosstabs and Pivot Tables\n",
    "\n",
    "In addition to groupby, spark provides a couple other ways to do aggregation.\n",
    "One of which is `.crosstab`. This is very similary to pandas `.crosstab`\n",
    "function, in that it calculates the number of occurances of each unique value\n",
    "from the two passed columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 143:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------+--------+--------+--------+--------+--------+--------+----------+----------+\n",
      "|cyl_trans|auto(av)|auto(l3)|auto(l4)|auto(l5)|auto(l6)|auto(s4)|auto(s5)|auto(s6)|manual(m5)|manual(m6)|\n",
      "+---------+--------+--------+--------+--------+--------+--------+--------+--------+----------+----------+\n",
      "|        8|       0|       0|      30|      17|       4|       1|       1|       5|         5|         7|\n",
      "|        5|       0|       0|       0|       0|       0|       0|       0|       2|         2|         0|\n",
      "|        6|       3|       0|      29|      16|       2|       0|       1|       5|        18|         5|\n",
      "|        4|       2|       2|      24|       6|       0|       2|       1|       4|        33|         7|\n",
      "+---------+--------+--------+--------+--------+--------+--------+--------+--------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mpg.crosstab('cyl', 'trans').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.crosstab` simply does counts, if we want a different aggregation, we can use\n",
    "`.pivot`. For example, to find the average highway mileage for each\n",
    "combination of car class and number of cylinders, we could write the\n",
    "following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 159:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----+------------------+------------------+\n",
      "|     class|                 4|   5|                 6|                 8|\n",
      "+----------+------------------+----+------------------+------------------+\n",
      "|subcompact| 30.80952380952381|28.5|24.714285714285715|              21.6|\n",
      "|   compact|          29.46875|29.0|25.307692307692307|              null|\n",
      "|   minivan|              24.0|null|              22.2|              null|\n",
      "|       suv|             23.75|null|              18.5|16.789473684210527|\n",
      "|   midsize|           29.1875|null| 26.26086956521739|              24.0|\n",
      "|    pickup|20.666666666666668|null|              17.9|              15.8|\n",
      "|   2seater|              null|null|              null|              24.8|\n",
      "+----------+------------------+----+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# so rather than passing two columns into our groupby\n",
    "# in this instance, we will indeed chain the pivot \n",
    "# into the second method of our sequence here.\n",
    "mpg.groupby('class').pivot('cyl').mean('hwy').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the unique values from the column we group by will be the rows in the\n",
    "resulting dataframe, and the unique values from the column we pivot on will\n",
    "become the columns. The values in each cell will be equal to the aggregation\n",
    "we specified over the group of values defined by the intersection of the rows\n",
    "and the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "Let's take a look at how spark handles missing data. First we'll create a dataframe that has a few missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|NaN|\n",
      "|2.0|0.0|\n",
      "|NaN|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|NaN|NaN|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = session.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\"x\": [1, 2, np.nan, 4, 5, np.nan], \"y\": [np.nan, 0, 0, 3, 1, np.nan]}\n",
    "    )\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides two main ways to deal with missing values:\n",
    "\n",
    "- `.fill`: to replace missing values with a specified value\n",
    "- `.drop`: to drop rows containing missing values\n",
    "\n",
    "Both methods are accessed through the `.na` property. We'll look at some examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|0.0|\n",
      "|2.0|0.0|\n",
      "|0.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|0.0|0.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|0.0|\n",
      "|2.0|0.0|\n",
      "|1.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|1.0|0.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0, subset='y').na.fill(1, subset='x').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|NaN|\n",
      "|2.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset='x').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that above the na values in the `x` column were filled with 0, but the na values in y were left alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the rows that had an na value for the y column were dropped, but the rows with na values for only the x column are still present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|2.0|0.0|\n",
      "|0.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0, subset='x').na.drop(subset='y').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withColumn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-----+----+---+----------+---+---+---+-------+-------------+\n",
      "|manufacturer|             model|displ|year|cyl|     trans|drv|cty| fl|  class|highway_again|\n",
      "+------------+------------------+-----+----+---+----------+---+---+---+-------+-------------+\n",
      "|        audi|                a4|  1.8|1999|  4|  auto(l5)|  f| 18|  p|compact|           30|\n",
      "|        audi|                a4|  1.8|1999|  4|manual(m5)|  f| 21|  p|compact|           30|\n",
      "|        audi|                a4|  2.0|2008|  4|manual(m6)|  f| 20|  p|compact|           32|\n",
      "|        audi|                a4|  2.0|2008|  4|  auto(av)|  f| 21|  p|compact|           31|\n",
      "|        audi|                a4|  2.8|1999|  6|  auto(l5)|  f| 16|  p|compact|           27|\n",
      "|        audi|                a4|  2.8|1999|  6|manual(m5)|  f| 18|  p|compact|           27|\n",
      "|        audi|                a4|  3.1|2008|  6|  auto(av)|  f| 18|  p|compact|           28|\n",
      "|        audi|        a4 quattro|  1.8|1999|  4|manual(m5)|  4| 18|  p|compact|           27|\n",
      "|        audi|        a4 quattro|  1.8|1999|  4|  auto(l5)|  4| 16|  p|compact|           26|\n",
      "|        audi|        a4 quattro|  2.0|2008|  4|manual(m6)|  4| 20|  p|compact|           29|\n",
      "|        audi|        a4 quattro|  2.0|2008|  4|  auto(s6)|  4| 19|  p|compact|           28|\n",
      "|        audi|        a4 quattro|  2.8|1999|  6|  auto(l5)|  4| 15|  p|compact|           26|\n",
      "|        audi|        a4 quattro|  2.8|1999|  6|manual(m5)|  4| 17|  p|compact|           26|\n",
      "|        audi|        a4 quattro|  3.1|2008|  6|  auto(s6)|  4| 17|  p|compact|           26|\n",
      "|        audi|        a4 quattro|  3.1|2008|  6|manual(m6)|  4| 15|  p|compact|           26|\n",
      "|        audi|        a6 quattro|  2.8|1999|  6|  auto(l5)|  4| 15|  p|midsize|           25|\n",
      "|        audi|        a6 quattro|  3.1|2008|  6|  auto(s6)|  4| 17|  p|midsize|           26|\n",
      "|        audi|        a6 quattro|  4.2|2008|  8|  auto(s6)|  4| 16|  p|midsize|           24|\n",
      "|   chevrolet|c1500 suburban 2wd|  5.3|2008|  8|  auto(l4)|  r| 14|  r|    suv|           21|\n",
      "|   chevrolet|c1500 suburban 2wd|  5.3|2008|  8|  auto(l4)|  r| 11|  e|    suv|           16|\n",
      "+------------+------------------+-----+----+---+----------+---+---+---+-------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumn is very similar to pd.DataFrame.assign()\n",
    "# drop, as with na above, behaves similarly to pandas pd.DataFrame.drop()\n",
    "mpg.withColumn('highway_again', F.col('hwy') + 1).drop('hwy').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Dataframe Manipulation Examples\n",
    "\n",
    "Let's take a look at some more examples of working with spark dataframes. For\n",
    "these examples, we'll be working with a dataset of observations of the\n",
    "weather in seattle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "|2012-01-01|          0.0|    12.8|     5.0| 4.7|drizzle|\n",
      "|2012-01-02|         10.9|    10.6|     2.8| 4.5|   rain|\n",
      "|2012-01-03|          0.8|    11.7|     7.2| 2.3|   rain|\n",
      "|2012-01-04|         20.3|    12.2|     5.6| 4.7|   rain|\n",
      "|2012-01-05|          1.3|     8.9|     2.8| 6.1|   rain|\n",
      "|2012-01-06|          2.5|     4.4|     2.2| 2.2|   rain|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vega_datasets import data\n",
    "\n",
    "weather = data.seattle_weather().assign(date=lambda df: df.date.astype(str))\n",
    "weather = session.createDataFrame(weather)\n",
    "weather.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the number of rows and columns in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, precipitation: double, temp_max: double, temp_min: double, wind: double, weather: string]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'precipitation', 'temp_max', 'temp_min', 'wind', 'weather']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1461, 6)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape metrics of the spark df:\n",
    "weather.count(), len(weather.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first find the dates where the data starts and stops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Column<'min(date)'>, Column<'max(date)'>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_date, max_date = F.min(weather.date), F.max(weather.date)\n",
    "min_date, max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(min(date)='2012-01-01', max(date)='2015-12-31')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.select(min_date, max_date).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use `.select` to select the minimum date and the maximum date.\n",
    "`.first` returns us the first row of our results, which consists of two value,\n",
    "and so can be unpacked into the `min_date` and `max_date` variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will combine the temp max and min columns into a single column,\n",
    "`temp_avg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "|2012-01-01|          0.0|    12.8|     5.0| 4.7|drizzle|\n",
      "|2012-01-02|         10.9|    10.6|     2.8| 4.5|   rain|\n",
      "|2012-01-03|          0.8|    11.7|     7.2| 2.3|   rain|\n",
      "|2012-01-04|         20.3|    12.2|     5.6| 4.7|   rain|\n",
      "|2012-01-05|          1.3|     8.9|     2.8| 6.1|   rain|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn is similar to our df.assign()\n",
    "weather = weather.withColumn(\n",
    "'temp_avg',\n",
    "    (weather.temp_min + weather.temp_max) / 2).drop(\n",
    "'temp_min', 'temp_max')\n",
    "# we did not reassign weather to the .show() version,\n",
    "# which is essentially assigning a nonetype the same type of way\n",
    "# that assigning a print statement would do\n",
    "# we are assigning the set of changing instructions to weather\n",
    "# so now when we pass show, these constructions apply as thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('date', 'string'),\n",
       " ('precipitation', 'double'),\n",
       " ('wind', 'double'),\n",
       " ('weather', 'string'),\n",
       " ('temp_avg', 'double')]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----+-------+------------------+\n",
      "|      date|precipitation|wind|weather|          temp_avg|\n",
      "+----------+-------------+----+-------+------------------+\n",
      "|2012-01-01|          0.0| 4.7|drizzle|               8.9|\n",
      "|2012-01-02|         10.9| 4.5|   rain| 6.699999999999999|\n",
      "|2012-01-03|          0.8| 2.3|   rain|              9.45|\n",
      "|2012-01-04|         20.3| 4.7|   rain| 8.899999999999999|\n",
      "|2012-01-05|          1.3| 6.1|   rain|              5.85|\n",
      "|2012-01-06|          2.5| 2.2|   rain|3.3000000000000003|\n",
      "|2012-01-07|          0.0| 2.3|   rain|               5.0|\n",
      "|2012-01-08|          0.0| 2.0|    sun|               6.4|\n",
      "|2012-01-09|          4.3| 3.4|   rain|               7.2|\n",
      "|2012-01-10|          1.0| 3.4|   rain|3.3499999999999996|\n",
      "+----------+-------------+----+-------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the total amount of rainfall for each month. We'll do\n",
    "this by first creating a month column, then grouping by the month, and\n",
    "finally, aggregating by taking the sum of the precipitation. To do this we will need to use the `month` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import month, year, quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 206:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------------------+\n",
      "|month|weather|        total_rain|\n",
      "+-----+-------+------------------+\n",
      "|    5|drizzle|               0.0|\n",
      "|    1|drizzle|               0.0|\n",
      "|    3|drizzle|               0.0|\n",
      "|    6|drizzle|               0.0|\n",
      "|    7|drizzle|               0.0|\n",
      "|    2|drizzle|               0.0|\n",
      "|   11|drizzle|               0.0|\n",
      "|    8|drizzle|               0.0|\n",
      "|   10|drizzle|               0.0|\n",
      "|    9|drizzle|               0.0|\n",
      "|   12|drizzle|               0.0|\n",
      "|    9|   rain|0.8999999999999999|\n",
      "|    4|drizzle|               1.0|\n",
      "|    7|    fog|2.5999999999999996|\n",
      "|    2|    sun|               4.0|\n",
      "|    4|   snow|               4.6|\n",
      "|    2|   snow|               5.7|\n",
      "|    5|    sun|               6.1|\n",
      "|    3|    sun| 8.899999999999999|\n",
      "|   11|    sun|              16.2|\n",
      "+-----+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "weather.withColumn(\n",
    "    'month', F.month(F.col('date')\n",
    "                    )\n",
    ").groupBy(\n",
    "    'month', 'weather'\n",
    ").agg(\n",
    "    F.sum(weather.precipitation).alias('total_rain')\n",
    ").sort('total_rain').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.sort` at the end isn't necessary, but presents that data in a friendlier\n",
    "way.\n",
    "\n",
    "Let's now take a look at the average temperature for each type of weather in\n",
    "December 2013:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month will rip out the month numeral from the date\n",
    "# establish a filter df[df.month == 12]\n",
    "# pass a second filter of df[df.year == 2013]\n",
    "# aggregate based on weather tag, present average temperature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we first have a couple of `.filter` calls in order to restrict our data\n",
    "to December of 2013. We then group by the weather column, and lastly,\n",
    "aggregate by taking the average of our `temp_avg` column. The combination of\n",
    "group by and agg will calculate the average temperature for each unique value\n",
    "of the `weather` column.\n",
    "\n",
    "Let's now find out how many days had freezing temperatures in each month of\n",
    "2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last example, let's calculate the average temperature for each quarter of\n",
    "each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the `quarter` and `year` columns, then group by these two new columns, and take the average temperature as our aggregate. Lastly, we sort by the year and quarter for presentation purposes.\n",
    "\n",
    "We could also use a pivot table like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here instead of grouping by two columns, we grouped by the first column and pivoted on the other column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "Like pandas and sql, spark has functionality that lets us combine two tabular\n",
    "datasets, known as a **join**.\n",
    "\n",
    "We'll start by creating some data that we can join together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- users ---\n",
      "+---+-----+-------+\n",
      "| id| name|role_id|\n",
      "+---+-----+-------+\n",
      "|  1|  bob|    1.0|\n",
      "|  2|  joe|    2.0|\n",
      "|  3|sally|    3.0|\n",
      "|  4| adam|    3.0|\n",
      "|  5| jane|    NaN|\n",
      "|  6| mike|    NaN|\n",
      "+---+-----+-------+\n",
      "\n",
      "--- roles ---\n",
      "+---+---------+\n",
      "| id|     name|\n",
      "+---+---------+\n",
      "|  1|    admin|\n",
      "|  2|   author|\n",
      "|  3| reviewer|\n",
      "|  4|commenter|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = session.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2, 3, 4, 5, 6],\n",
    "            \"name\": [\"bob\", \"joe\", \"sally\", \"adam\", \"jane\", \"mike\"],\n",
    "            \"role_id\": [1, 2, 3, 3, np.nan, np.nan],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "roles = session.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2, 3, 4],\n",
    "            \"name\": [\"admin\", \"author\", \"reviewer\", \"commenter\"],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "print(\"--- users ---\")\n",
    "users.show()\n",
    "print(\"--- roles ---\")\n",
    "roles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To join two dataframes together, we'll need to call the `.join` method on one\n",
    "of them and supply the other as an argument. In addition, we'll need to supply\n",
    "the condition on which we are joining. In our case, we are joining where the\n",
    "`role_id` column on the users table is equal to the `id` column on the roles\n",
    "table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+----+--------+\n",
      "| id| name|role_id|  id|    name|\n",
      "+---+-----+-------+----+--------+\n",
      "|  1|  bob|    1.0|   1|   admin|\n",
      "|  2|  joe|    2.0|   2|  author|\n",
      "|  3|sally|    3.0|   3|reviewer|\n",
      "|  4| adam|    3.0|   3|reviewer|\n",
      "|  5| jane|    NaN|null|    null|\n",
      "|  6| mike|    NaN|null|    null|\n",
      "+---+-----+-------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.join(roles, how='left', on=users.role_id == roles.id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, spark will perform an inner join, meaning that records from both\n",
    "dataframes will have a match with the other. We can also specify either a left\n",
    "or a right join, which will keep all of the records from either the left or\n",
    "right side, even if those records don't have a match with the other dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that examples above have a duplicate `id` column. There are several\n",
    "ways we could go about dealing with this:\n",
    "\n",
    "- alias each dataframe + explicitly select columns after joining (this could also be implemented with spark SQL)\n",
    "- rename duplicated columns before merging\n",
    "- drop duplicated columns after the merge (`.drop(right.id)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization (or Lack Therof)\n",
    "\n",
    "Spark does not provide a way to do visualization with their dataframes. To\n",
    "visualize data from spark, you should use the `.toPandas` method on a spark\n",
    "dataframe to convert it to a pandas dataframe, then visualize as you normally\n",
    "would.\n",
    "\n",
    "!!!warning \"Converting to A Pandas Dataframe\"\n",
    "    Converting a spark dataframe to a pandas dataframe will pull all the data into memory, so make sure you have enough available memory to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>model</th>\n",
       "      <th>displ</th>\n",
       "      <th>year</th>\n",
       "      <th>cyl</th>\n",
       "      <th>trans</th>\n",
       "      <th>drv</th>\n",
       "      <th>cty</th>\n",
       "      <th>hwy</th>\n",
       "      <th>fl</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>4</td>\n",
       "      <td>auto(l5)</td>\n",
       "      <td>f</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>4</td>\n",
       "      <td>manual(m5)</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>manual(m6)</td>\n",
       "      <td>f</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>auto(av)</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audi</td>\n",
       "      <td>a4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>6</td>\n",
       "      <td>auto(l5)</td>\n",
       "      <td>f</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>p</td>\n",
       "      <td>compact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>passat</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>auto(s6)</td>\n",
       "      <td>f</td>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>p</td>\n",
       "      <td>midsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>passat</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>manual(m6)</td>\n",
       "      <td>f</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>p</td>\n",
       "      <td>midsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>passat</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>6</td>\n",
       "      <td>auto(l5)</td>\n",
       "      <td>f</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>p</td>\n",
       "      <td>midsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>passat</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1999</td>\n",
       "      <td>6</td>\n",
       "      <td>manual(m5)</td>\n",
       "      <td>f</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>p</td>\n",
       "      <td>midsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>passat</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "      <td>auto(s6)</td>\n",
       "      <td>f</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>p</td>\n",
       "      <td>midsize</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    manufacturer   model  displ  year  cyl       trans drv  cty  hwy fl  \\\n",
       "0           audi      a4    1.8  1999    4    auto(l5)   f   18   29  p   \n",
       "1           audi      a4    1.8  1999    4  manual(m5)   f   21   29  p   \n",
       "2           audi      a4    2.0  2008    4  manual(m6)   f   20   31  p   \n",
       "3           audi      a4    2.0  2008    4    auto(av)   f   21   30  p   \n",
       "4           audi      a4    2.8  1999    6    auto(l5)   f   16   26  p   \n",
       "..           ...     ...    ...   ...  ...         ...  ..  ...  ... ..   \n",
       "229   volkswagen  passat    2.0  2008    4    auto(s6)   f   19   28  p   \n",
       "230   volkswagen  passat    2.0  2008    4  manual(m6)   f   21   29  p   \n",
       "231   volkswagen  passat    2.8  1999    6    auto(l5)   f   16   26  p   \n",
       "232   volkswagen  passat    2.8  1999    6  manual(m5)   f   18   26  p   \n",
       "233   volkswagen  passat    3.6  2008    6    auto(s6)   f   17   26  p   \n",
       "\n",
       "       class  \n",
       "0    compact  \n",
       "1    compact  \n",
       "2    compact  \n",
       "3    compact  \n",
       "4    compact  \n",
       "..       ...  \n",
       "229  midsize  \n",
       "230  midsize  \n",
       "231  midsize  \n",
       "232  midsize  \n",
       "233  midsize  \n",
       "\n",
       "[234 rows x 11 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Using the [repo setup directions](https://ds.codeup.com/fundamentals/git/), setup a new local and remote repository named `spark-exercises`. The local version of your repo should live inside of `~/codeup-data-science`. This repo should be named `spark-exercises`\n",
    "\n",
    "Save this work in your `spark-exercises` repo. Then add, commit, and push your changes.\n",
    "\n",
    "Create a jupyter notebook or python script named `spark101` for this exercise.\n",
    "\n",
    "1. Create a spark data frame that contains your favorite programming languages.\n",
    "\n",
    "    - The name of the column should be `language`\n",
    "    - View the schema of the dataframe\n",
    "    - Output the shape of the dataframe\n",
    "    - Show the first 5 records in the dataframe\n",
    "\n",
    "1. Load the `mpg` dataset as a spark dataframe.\n",
    "\n",
    "    1. Create 1 column of output that contains a message like the one below:\n",
    "\n",
    "            The 1999 audi a4 has a 4 cylinder engine.\n",
    "\n",
    "        For each vehicle.\n",
    "\n",
    "    1. Transform the `trans` column so that it only contains either `manual` or `auto`.\n",
    "\n",
    "1. Load the `tips` dataset as a spark dataframe.\n",
    "\n",
    "    1. What percentage of observations are smokers?\n",
    "    1. Create a column that contains the tip percentage\n",
    "    1. Calculate the average tip percentage for each combination of sex and smoker.\n",
    "\n",
    "1. Use the seattle weather dataset referenced in the lesson to answer the questions below.\n",
    "\n",
    "    - Convert the temperatures to fahrenheit.\n",
    "    - Which month has the most rain, on average?\n",
    "    - Which year was the windiest?\n",
    "    - What is the most frequent type of weather in January?\n",
    "    - What is the average high and low temperature on sunny days in July in 2013 and 2014?\n",
    "    - What percentage of days were rainy in q3 of 2015?\n",
    "    - For each year, find what percentage of days it rained (had non-zero precipitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
